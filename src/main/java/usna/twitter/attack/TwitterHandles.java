package usna.twitter.attack;

/**
 * 
 * TwitterHandles trains a LinearClassifier on training set of usernames: usernames-training.txt,
 * and uses that LinearClassifier to predict entity-name and alias matches on a test set of
 * usernames: usernames-test.txt. The code outputs a table detailing the accuracy of comparisons
 * between entity names and handles.
 * 
 * There are three primary methods called from main():
 * 	- load(): loads the countsFile, vectorFile, and matrixFile from the pathnames provided in the
 * 	arguments. These files are loaded into HashMaps to be used by various methods in the code.
 * 	- train(): uses featsIntoCounter() to train the LinearClassifier over all entity/handle pairs
 * 	in usernames-train.txt. Weights are assigned to each feature based on its usefulness in
 * 	predicting a match.
 * 	- test(): uses isMatch() to predict a match/!match between an entity/handle pair. The output
 * 	of the code is generated in this method.
 * 
 * NOTES**: if you are not using the averageVectors() method, comment out the call to averageVectors()
 * at the top of the train() and test() methods. averageVectors() is very expensive and takes a long
 * time to execute.
 * NOTES**: if you are not using word vector features (e.g. wordVectorBinary()), comment out the
 * loadMap() method in load(). This also consumes a significant amount of time.
 * 
 * <trainingFile>: usernames-training.txt: this file contains the
 *	usernames used to train the LinearClassifier. Typically located
 *	in navynlp/
 *	
 *	<testFile>: usernames-devs.txt and usernames-test.txt: these files
 *	contain the development set and testing set of usernames. Use
 *	development set while testing/debugging the program to maintain the
 *	integrity of the test. Typically located in navnlp/
 *	
 *	<vectorFile>: mergedVectors.all: contains the merged vectors and is
 *	used in all wordVector features. This file is read into "map" in the
 *	loadMap() method. Typically located in userWork/mergedVectors.all or
 *  /scratch/nchamber/merged-context-vectors.vecs
 *  This file contains word vectors for each entity in usernames-all.txt.
 *	
 *	<countsFile>: mergedCounts.all: contains the merged username counts
 *	and is used in the "test" method. This file is read into "counts" in
 *	the loadCounts() method. Located in the same directory as mergedVectors
 *	In essence, this file contains key/value pairs, where the key is an
 *  entity from usernames-all.txt, and the value is the number of times each
 *  key was mentioned in the tweets.
 *
 *	<matrixFile>: deepMatrix4.txt: usually located in the
 *	/code/navynlp/deep directory. This file contains the matrices
 *	generated by the deepLearning4java code. This file is read into
 *	"nnmap" in the "loadDL4J" method and is used in dl4j().
 *
 * 
 * HOW TO RUN:
 * java usna.twitter.attack.TwitterHandles trainingFile testingFile vectorFile countFile matrixFile
 * 
 * SCRIPT (use an editor to change the arguments as needed):
 * code/navynlp/scripts/runHandles
 * 
 * 
 */

import java.io.BufferedReader;
import java.io.File;
import java.io.FileInputStream;
import java.io.FileReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.zip.GZIPInputStream;

import edu.stanford.nlp.ling.RVFDatum;
import edu.stanford.nlp.stats.ClassicCounter;
import edu.stanford.nlp.stats.Counter;
import edu.stanford.nlp.stats.Counters;
import edu.stanford.nlp.classify.LinearClassifier;
import edu.stanford.nlp.classify.LinearClassifierFactory;
import edu.stanford.nlp.classify.RVFDataset;
import edu.stanford.nlp.util.StringUtils;

public class TwitterHandles {

	/* Globals */
	Map<String, Counter<String>> twitter_handles = new HashMap<String, Counter<String>>(); // *** DEBUGGING TOOL ***
	ArrayList<String> entityHandlePairs = new ArrayList<String>(); // *** DEBUGGING TOOL ***
	int numFiles = 99999999; // number of maps to red *** DEBUGGING TOOL ***

	// List of common prepositions used in several features
	List<String> prepositions = Arrays.asList("on", "in", "at", "for", "to",
			"by", "of", "since");

	// Linear classifier takes weights and determines match/notmatch
	LinearClassifier<String, String> myClassifier;
	int totalGuessedMatch = 0;

	// Maps to store mergedVectors, mergedCounts, matrix file, and average vectors, respectively.
	HashMap<String, Counter<String>> nameVectors = new HashMap<String, Counter<String>>();
	HashMap<String, Double> counts = new HashMap<String, Double>();
	HashMap<String, Counter<String>> embeddings = new HashMap<String, Counter<String>>();
	HashMap<String, Double> avgMap = new HashMap<String, Double>();

	boolean usingVecs = true;
	int totVecCount = 0; // equal to size of usernames-test set. Used in wordVector() features
	int vecCount = 0; // tracks the total number of entity/handle pairs that both have word vectors
	/* End Globals */

	/**
	 * Internal class for storing the training/test data format.
	 */
	public class EntityHandle {
		public String entity; // e.g. "Chase Bank"
		public String handle; // e.g. "@chaseBank"
		boolean correct;

		public EntityHandle(String e, String h, boolean c) {
			entity = e;
			handle = h;
			correct = c;
		}
	}

	// Not sure why this is needed. Nothing is ever added to twitter_handles. Used for debugging.
	public Map<String, Counter<String>> getTwitterHandles() {
		return twitter_handles;
	}

	/*
	 *************************
	 * THE FOLLOWING METHODS ARE THE FEATURES USED IN featsIntoCounter(). THEY PROVIDE AN ESTIMATION
	 * OF THE LIKELIHOOD OF A MATCH BETWEEN AN ENTITY AND A POSSIBLEHANDLE. SEVERAL FEATURES MAY
	 * NEED TO BE TWEAKED IN ORDER TO MAXIMIZE ACCURACY.
	 * ***********************
	 */

	/* 
	 * hasSemiAcronym: takes an entity "B of A" and possible_handle "BankOfAmerica"
	 * and parses the entity to determine if the entity is a partial acronym of
	 * the handle
	 * ***** NEEDS WORK *****
	 */
	public int hasSemiAcronym(String entity, String possible_handle) {

		possible_handle = possible_handle.toLowerCase().replace("@", "");
		String[] words = entity.toLowerCase().split("\\s+");
		String entAcr = "";		
		int minAcrSize = 3;

		// Sanity Check
		if (entity.length() <= 1 || possible_handle.length() <= 1) { return 0; }

		// Loop over each word in entity and check for prepositions
		for (String word : words) {
			if (prepositions.contains(word))
				entAcr += word; // add the full preposition
			else
				entAcr += word.substring(0, 1); // add only the first letter	
		}

		// Successful Test
		if (possible_handle.contains(entAcr) && entAcr.length() >= minAcrSize) {
			return entAcr.length();
		}

		// Failed test
		return 0;

	}

	/*
	 * is_acronym: takes an entity "fb" and a possible_handle "@facebook" and
	 * attempts to determine whether the entity is an acronym of the handle.
	 * ***** NEEDS WORK ******
	 * ***** ISSUE: handles are one word. Impossible to split on whitespace. Dictionary? *****
	 */
	public int is_acronym(String entity, String possible_handle) {

		// Clean up the input and prep it for processing
		possible_handle = possible_handle.toLowerCase().replace("@", "");
		String[] words = entity.toLowerCase().split("\\s+");
		entity = entity.replaceAll("\bthe\b", " ");
		possible_handle = possible_handle.replaceAll("\bthe\b", " ");
		int which_word = 0;

		// Sanity Checks
		if (entity.length() <= 1 || possible_handle.length() <= 1) { return 0; }
		if (words.length != (possible_handle.length())) { return 0; }

		/* Check if the first letter of the Nth word matches the Nth
		 * character of the handle
		 */
		for (String word : words) {
			try {
				if (possible_handle.charAt(which_word) != word.charAt(0))
					return 0;
			} catch (StringIndexOutOfBoundsException e) {
				return 0;
			}
			which_word++;
		}

		return 1; // success
	}

	/*
	 * first_name_substring: checks to see if the entity is the first word in
	 * a handle. e.g: "Tyler" is "TylerTheCreator"
	 * ***** SHOULD BE OKAY *****
	 */
	public int first_name_substring(String entity, String possible_handle) {
		int minWordLength = 3;
		entity = entity.toLowerCase();
		possible_handle = possible_handle.toLowerCase().replace("@","");
		entity = entity.replaceAll("\bthe\b", " ");
		possible_handle = possible_handle.replaceAll("\bthe\b", " ");
		entity = entity.replaceAll(".com", " ");
		possible_handle = possible_handle.replaceAll("_com", " ");
		possible_handle = possible_handle.trim();
		entity = entity.trim();

		// Sanity Check
		if (entity.length() <= 1 || possible_handle.length() <= 1) { return 0; }

		String first_word = entity.split("\\s+")[0];

		if (possible_handle.startsWith(first_word) && first_word.length() >= minWordLength)
			return first_word.length();

		return 0;
	}

	/*
	 * last_name_substring: checks to see if the entity is the last word of
	 * a handle.
	 * ***** MIGHT NEED SOME WORK, SPECIFICALLY REGARDING THE "@" SYMBOL *****
	 */
	public int last_name_substring(String entity, String possibleHandle) {
		int minWordLength = 3;
		entity = entity.toLowerCase();
		possibleHandle = possibleHandle.toLowerCase().replace("@","");
		entity = entity.replaceAll("\\.com", " ");
		possibleHandle = possibleHandle.replaceAll("_com", " ");
		possibleHandle = possibleHandle.trim();
		entity = entity.trim();

		if (entity.length() <= 1 || possibleHandle.length() <= 1) { return 0; }

		String last_word = entity.split("\\s+")[entity.split("\\s+").length - 1];

		if (possibleHandle.endsWith(last_word) && last_word.length() >= minWordLength)
			return last_word.length();

		return 0;
	}

	/*
	 * firstNLetterMatch: returns the number of first letter matches when comparing
	 * an entity to a possiblehandle.
	 * e.g: "insta" / "@instagram" returns 5
	 * **** ISSUE: Eliminate all the "the"? ****
	 */
	public double firstNLetterMatch(String entity, String possibleHandle) {
		entity = entity.toLowerCase();
		entity = entity.replaceAll("\bthe\b","").replaceAll("\\s+","").trim();
		possibleHandle = possibleHandle.toLowerCase().replace("@","");
		possibleHandle = possibleHandle.replaceAll("\bthe\b","").trim();
		double count = 0.0; // initialize count

		// Sanity check
		if (entity.length() < 1 || possibleHandle.length() < 1) { return 0.0; }

		if (entity.length() <= possibleHandle.length()) {
			for (int ii = 0; ii < entity.length(); ii++) {
				if (entity.charAt(ii) == possibleHandle.charAt(ii))
					count++;
				else
					return count;
			}
		} else {
			for (int ii = 0; ii < possibleHandle.length(); ii++) {
				if (entity.charAt(ii) == possibleHandle.charAt(ii))
					count++;
				else
					return count;
			}
		}

		return count;
	}

	/*
	 * lastNLetterMatch: performs the same function as firstNLetterMatch, at the
	 * tail of the words
	 * **** ISSUE: Eliminate all the "the"? ****
	 */
	public double lastNLetterMatch(String entity, String possibleHandle) {
		entity = entity.toLowerCase();
		entity = entity.replaceAll("\bthe\b", "");
		entity = entity.replaceAll("\\s+", "");
		possibleHandle = possibleHandle.toLowerCase();
		possibleHandle = possibleHandle.replaceAll("\bthe\b", "").replace("@","");
		possibleHandle = possibleHandle.trim();
		entity = entity.trim();
		double count = 0.0; // initialize count

		// Sanity check
		if (entity.length() < 1 || possibleHandle.length() < 1) { return 0.0; }

		if (entity.length() <= possibleHandle.length()) {
			for (int ii = 0; ii < entity.length(); ii++) {
				if (entity.charAt(entity.length() - (ii + 1)) == possibleHandle
						.charAt(possibleHandle.length() - (ii + 1)))
					count++;
				else
					return count;
			}
		} else {
			for (int ii = 0; ii < possibleHandle.length(); ii++) {
				if (entity.charAt(entity.length() - (ii + 1)) == possibleHandle
						.charAt(possibleHandle.length() - (ii + 1)))
					count++;
				else
					return count;
			}
		}

		return count;
	}

	/*
	 * capitalization: grabs the capital letters of the entity and handle
	 * and compares them. Returns true only on an identical match
	 * **** ISSUE: MAY BE OVERFITTING BY INCLUDING DIGITS IN HERE!! ****
	 * **** ISSUE: Should prepositions be taken into consideration? ****
	 */
	public int capitalization(String entity, String possibleHandle) {

		// Grabs all capital letters from the entity
		ArrayList<Character> capitalLettersEnt = new ArrayList<Character>();
		for (char c : entity.toCharArray()) {
			if (Character.isUpperCase(c) || Character.isDigit(c))
				capitalLettersEnt.add(c);
		}

		// Grabs all capital letters from the handle
		ArrayList<Character> capitalLettersHandle = new ArrayList<Character>();
		for (char c : possibleHandle.toCharArray()) {
			if (Character.isUpperCase(c) || Character.isDigit(c))
				capitalLettersHandle.add(c);
		}

		// If the sizes differ, return false
		if (capitalLettersHandle.size() != capitalLettersEnt.size())
			return 0;

		// If the actual letters differ, return false
		for (int ii = 0; ii < capitalLettersHandle.size(); ii++) {
			if (capitalLettersHandle.get(ii) != capitalLettersEnt.get(ii))
				return 0;
		}

		return 1;
	}

	/*
	 * capitalAcronym: grabs the capital letters from an entity and compares them to
	 * the handle. Returns the number of matches
	 */
	public int capitalAcronym(String entity, String handle) {
		// Grabs all capital letters from the entity
		ArrayList<Character> capitalLettersEnt = new ArrayList<Character>();
		for (char c : entity.toCharArray()) {
			if (Character.isUpperCase(c) || Character.isDigit(c))
				capitalLettersEnt.add(c);
		}

		handle = handle.toLowerCase();

		int score = 0;

		String capEnt = capitalLettersEnt.toString().toLowerCase();

		for (int i = 0; i < capEnt.length(); i++)
			if (handle.contains(capEnt.substring(i,i++)))
				score++;

		return score;
	}

	/*
	 * capitalSubstring: splits the entity into substrings based on capital
	 * letters and checks to see if the handle contains any substrings of the
	 * split entity.
	 * *** SHOULD BE OKAY ***
	 */
	public int capitalSubstring(String entity, String handle) {
		int numContains = 0;
		//String[] entSplit = entity.split("(?<=[a-z])(?=[A-Z])");
		String[] entSplit = entity.split("[^a-zA-Z0-9]");

		for (int i = 0; i < entSplit.length; i++) {
			if (handle.contains(entSplit[i]) && entSplit[i].length() > 2) { numContains = 1; }
		}

		return numContains; // false
	}

	/*
	 * percentSubstring: returns the percent of entity that is a substring
	 * of the handle.
	 * e.g: "face" / "facebook" returns 0.5
	 * **** ISSUE: Could be optimized in one submethod **** SEE CODE ****
	 */
	public double percentSubstring(String entity, String possibleHandle) {
		entity = entity.toLowerCase().replace(".com"," ").trim();
		possibleHandle = possibleHandle.toLowerCase().replace("_com"," ").trim();
		String[] entityArray = entity.split("\\s+");

		// Use toCharArray() to iterate over the string and find number of chars
		// *** ISSUE: could use entity.replace(" ","").length() to find numCharEnt?
		int numCharacterEnt = 0;
		for (char c : entity.toCharArray()) {
			if (c != ' ')
				numCharacterEnt++;
		}

		// Add any substrings to count
		int lettersThatAreSubstring = 0;
		for (String s : entityArray) {
			if (possibleHandle.contains(s))
				lettersThatAreSubstring += s.length();
		}

		int minWordLength = 4; // ideal length is 4-5
		if (numCharacterEnt >= minWordLength)
			return ((double)lettersThatAreSubstring / (double)numCharacterEnt);
		else
			return 0.0;
	}

	/*
	 * containsWholeEntity: returns true if the handle contains the entire
	 * entity.
	 * **** SHOULD BE OKAY ****
	 */
	public int containsWholeEntity(String entity, String possibleHandle) {

		entity = entity.toLowerCase();
		possibleHandle = possibleHandle.toLowerCase();

		int minEntSize = 1; // Ideal value is 1-3
		if (possibleHandle.contains(entity) && entity.length() >= minEntSize)
			return 1;

		return 0;

	}

	private String removeNonAlphaNumeric(String name) {
	  name = name.toLowerCase();
	  
	  String newname = "";
	  for( int i = 0; i < name.length(); i++ ) {
	    char ch = name.charAt(i);
	    if( ch >= 'a' && ch <= 'z' )
	      newname += ch;
	  }
	  
	  return newname;
	}

	public int exactMatchStartString(String entity, String handle, int length) {
	  entity = removeNonAlphaNumeric(entity);
	  handle = removeNonAlphaNumeric(handle);
	  entity = (entity.length() >= length ? entity.substring(0, length) : entity);
	  handle = (handle.length() >= length ? handle.substring(0, length) : handle);

	  if( entity.equals(handle) ) 
	    return 1;

	  return 0;
	}
	 
	public int exactMatchAlphaNumeric(String entity, String handle) {
	  entity = removeNonAlphaNumeric(entity);
	  handle = removeNonAlphaNumeric(handle);

	  if( entity.equals(handle) ) 
	    return 1;

	  return 0;
	}
	 
	/*
	 * exactMatch: returns true on exact match, false otherwise
	 * **** SHOULD BE OKAY ****
	 */
	public int exactMatch(String entity, String handle) {
		handle = handle.toLowerCase().replace("@","");
		entity = entity.toLowerCase();
		entity = entity.replaceAll("\\s+", "");

		if (entity.equals(handle)) 
		  return 1;

		return 0;
	}

	/*
	 * oneWordSubstring: returns true if the handle contains any
	 * word in the entity
	 * **** DECREASES ACCURACY **** DOUBTFUL OF ITS EFFECTIVENESS
	 */
	public int oneWordSubstring(String entity, String handle) {
		entity = entity.toLowerCase();
		handle = handle.toLowerCase();

		for (String word : entity.split("\\s+"))
			if (handle.contains(word)) { return entity.length(); }

		return 0;
	}

	/*
	 * reverseSubstring: returns true if the handle contains any of the
	 * entity's reversed substrings
	 */
	public int reverseSubstring(String entity, String handle) {
		int score = 0;
		for (String word : entity.split("\\s+")) {
			word = new StringBuilder(word).reverse().toString();
			if (handle.contains(word)) { score++; }
		}
		return score;
	}

	/*
	 * computeEditDistance: edit distance is a way of quantifying how 
	 * dissimilar two strings (e.g., words) are to one another by counting 
	 * the minimum number of operations required to transform one string 
	 * into the other. This is used as the baseline feature in testing.
	 */
	public double computeEditDistance(String entity, String possibleHandle) {
	  entity = entity.toLowerCase();
	  possibleHandle = possibleHandle.toLowerCase();
	  if( possibleHandle.startsWith("@") )
	    possibleHandle = possibleHandle.substring(1);
	  
//	  System.out.println(entity + " and " + possibleHandle + " = " + StringUtils.editDistance(entity, possibleHandle));
		return StringUtils.editDistance(entity, possibleHandle);
	}

	 /*
   * computeEditDistance: edit distance is a way of quantifying how 
   * dissimilar two strings (e.g., words) are to one another by counting 
   * the minimum number of operations required to transform one string 
   * into the other. This is used as the baseline feature in testing.
   */
  public double computeInverseEditDistance(String entity, String possibleHandle) {
    entity = entity.toLowerCase();
    possibleHandle = possibleHandle.toLowerCase();
    if( possibleHandle.startsWith("@") )
      possibleHandle = possibleHandle.substring(1);
    
    double edit = StringUtils.editDistance(entity, possibleHandle);
    double avg = edit / (entity.length() > 0.0 ? entity.length() : 1.0); 
    return 1.0 - avg;
  }

	/*
	 * wordVectorBinary: returns 1 if the cosine score is above 0.1, returns 0 otherwise
	 * This is the binary version of wordVector, which utilizes the vector hashmap.
	 */
	public int wordVectorBinary(String entity, String possibleHandle) throws IOException {
		int ret = 0;
		double comp = 0.0;
		totVecCount++;
		Counter<String> entVec = nameVectors.get(entity);
		Counter<String> possHanVec = nameVectors.get(possibleHandle);

		if (entVec != null && possHanVec != null) {
			comp = Counters.cosine(entVec,possHanVec);
			vecCount++;
		}

		if (comp > 0.1 /* && counts.get(entity)>100 */) {
			ret = 1;
			totalGuessedMatch++;
			// System.out.println("Guessed match: "+entity+" : "+possibleHandle);
		} // else {
		// System.out.println("NO MATCH: "+entity+" : "+possibleHandle);
		// }

		return ret;
	}

	/*
	 * wordVectorCosine: compares entities and handles based on word vectors.
	 * Returns the cosine of the two vectors.
	 */
	public double wordVectorCosine(String entity, String possibleHandle) throws IOException {

		Counter<String> entVec = nameVectors.get(entity);
		Counter<String> possHanVec = nameVectors.get(possibleHandle);

		//System.out.println(entity + "\t" + possibleHandle);

		//if (map.get(entity) != null) { System.out.println("e:\t" + entity + "\t" + possibleHandle); }

		//if (map.get(possibleHandle) != null) { System.out.println("p:\t" + entity + "\t" + possibleHandle); }

		double comp = 0.0;
		totVecCount++;

		if (entVec != null && possHanVec != null) {
			comp = Counters.cosine(entVec, possHanVec);
			vecCount++;
		}

		return comp;
	}

	/*
	 * wordVectorCosineAverage: compares entities and handles based on word vectors.
	 * Returns the ratio between the cosine comparison of entity/handle and the average
	 * cosine value of the entity when compared against all handles in the dataset. Scores
	 * revolve around "0.0" as a baseline. Positive values are correlated with an expected match,
	 * while negative values are indicative of a !match.
	 */
	public double wordVectorCosineAverage(String entity, String possibleHandle) throws IOException {

		Counter<String> entVec = nameVectors.get(entity);
		Counter<String> possHanVec = nameVectors.get(possibleHandle);

		//System.out.println(entity + "\t" + possibleHandle);

		//if (map.get(entity) != null) { System.out.println("e:\t" + entity + "\t" + possibleHandle); }

		//if (map.get(possibleHandle) != null) { System.out.println("p:\t" + entity + "\t" + possibleHandle); }

		double comp = 0.0;
		totVecCount++;

		if (avgMap.containsKey(entity)) {
			if (entVec != null && possHanVec != null) {
				comp = Counters.cosine(entVec, possHanVec);
				vecCount++;

				double avg = avgMap.get(entity);
				if( avg > 0.0 )
					comp = (comp/avgMap.get(entity)) - 1;
				else
					comp = 0.0;
			}	       
		}
		return comp;
	}

	/*
	 * dl4j: compares the deep-learning matrices of an entity/handle and
	 * returns their cosine value. The dl4j() method is used in place of
	 * wordVector() methods. See the deepLearning4j code for a better understanding
	 * of what is happening here. 
	 */
	public double dl4j(String entity, String possibleHandle) {

		Counter<String> entVec = embeddings.get(entity);
		Counter<String> possHanVec = embeddings.get(possibleHandle);

		double comp = 0.0;

		//if (possHanVec != null)
		//	System.out.println(possibleHandle + "\t" + possHanVec);

		if (entVec != null && possHanVec != null) {
			comp = Counters.cosine(entVec, possHanVec);
			//System.out.println(comp);
		}

		/*
		 * if (entity.contains("@justinbieber")) {
		 * System.out.println(nnmap.get(entity));
		 * System.out.println(nnmap.get(possibleHandle));
		 * System.out.println(entity + " " + possibleHandle + " : " + comp +
		 * "\n\n"); }
		 */

		return comp;
	}

	/*
	 *******************************************************
	 * END OF FEATURES METHODS.
	 * *****************************************************
	 */


	/*
	 * averageVector: finds the average cosine value of an entity when compared to
	 * all other entities in a list, with the condition that neither of these
	 * entities possess a null vector set.
	 * Fills a global HashMap "avgMap", returns void.
	 */
	public void averageVector(String filename) {

		System.out.println("Loading average vectors from " + filename); // sanity check
		List<EntityHandle> entities = getEntityData(filename); // stores EntityHandles from filename
		List<EntityHandle> entityList = new ArrayList<>(); // used to store not-null EntityHandle pairs
		int progressCounter = 0; // used to track progress through the list

		// If the entity and handle vectors are not null, store the EH pair in entityList
		// This ensures that no checks are being made against entities with null vector sets
		for ( EntityHandle eh : entities ) {
			if (nameVectors.get(eh.entity) != null && nameVectors.get(eh.handle) != null)
				entityList.add(eh);
		}

		// Print number of entities found
		System.out.println("Entities: " + entityList.size() + "\n");

		// Iterate over each element in the entityList and compute average cosine values
		for( EntityHandle eh : entityList ) {

			int i = 0; // denominator for average cosine value
			double comp = 0, totComp = 0, avgComp = 0;
			Counter<String> entVec = nameVectors.get(eh.entity);

			// Compare the first ENTITY with every HANDLE in entityList
			for ( EntityHandle eh2 : entityList ) {
				Counter<String> compVec = nameVectors.get(eh2.handle);
				comp = Counters.cosine(entVec, compVec);
				totComp += comp;
				i++;
			}

			// Compute average cosine value and store the entity/avg in avgMap
			avgComp = totComp / i;
			avgMap.put(eh.entity,avgComp);

			// Increment progress counter and output statement
			progressCounter++;
			if (progressCounter % 100 == 0)
				System.out.println("\nProgress: " + "\t" + progressCounter + "/" + entityList.size() + "\n");
		}

		System.out.println("Finished in averageVector()!!!"); // completed method
	}	


	/*
	 * countOfEnt: takes the HashMap of counts and finds the count (value) of each
	 * entity and handle if the entity and handle are in the HashMap. Returns
	 * true if the counts are above a given threshold (5).
	 * **** NOT ENTIRELY SURE WHY THIS IS NEEDED, NOT USED OUTSIDE OF DEBUGGING ****
	 */
	public int countOfEnt(String entity, String possibleHandle) {
		double entityCount = 0;
		double handleCount = 0;

		if (counts.containsKey(entity) && counts.containsKey(possibleHandle)) {
			entityCount = counts.get(entity);
			handleCount = counts.get(possibleHandle);
		} else { return 0; }

		if ((entityCount < 5) || (handleCount < 5)) { return 0; }

		return 1;
	}

	/*
	 * ****************************************
	 * BEGINNING OF LOAD METHODS. TAKES ALL ARGUMENTS TO THE PROGRAM AND
	 * LOADS THEIR DATA INTO THREE HASHMAPS
	 * ****************************************
	 */

	/*
	 * load: called in main and used to load the vectors (mapFile), matrices (nmapFile), and
	 * counts (countFile) into their respective HashMaps.
	 * These are expensive methods, be sure to turn them off if not needed
	 */
	public void load(String mapFile, String countFile, String nmapFile) {
	  if( usingVecs ) {
	    loadDL4J(nmapFile); // matrixFile args[4]
	    loadMap(mapFile); // vectorFile args[2]
	  }
	  else System.out.println("Not using word vectors. Skipping load.");
	  
	  loadCounts(countFile); // countFile args[3]
	  System.out.println("Finished loading all data structures...\n");
	}

	/* Takes nmapFile as an argument which contains a list of entities and words associated
	 * 	with said entities. Words contain a "count" or "weight" that describes frequency of
	 *  use. The file is read in line by line and is parsed into entity/line pairs. The
	 *  words and their "counts" are stored in a ClassicCounter <nw> which is then put into
	 *  nnmap.
	 */
	public void loadDL4J(String nmapFile) {
		System.out.println("Reading in the neural net map from " + nmapFile +"...");
		try {
			@SuppressWarnings("resource")
			BufferedReader file = new BufferedReader(new InputStreamReader(
					new FileInputStream(new File(nmapFile))));
			String line; // stores line
			int count = 0;

			while ((line = file.readLine()) != null) {
				count++;

				if (count % 5000 == 0) { System.out.println("Total: " + count); }

				Counter<String> nw = new ClassicCounter<String>();
				String[] splLine = line.split("\t"); // split on tabs
				String ent = splLine[0].trim(); // grab entity from split line

				// Puts each vector/value pair of an entity into a counter
				if (splLine.length > 1) {
					if (splLine.length % 2 == 1) { // if splLine is odd ** SANITY CHECK
						for (int i = 1, n = splLine.length; i < n; i += 2) {
							Double in = Double.parseDouble(splLine[i + 1]);
							nw.setCount(splLine[i].toString(), in);
						}
					}
				}

				embeddings.put(ent, nw);

				//System.out.println(nnmap.get(ent));
			}
		} catch (IOException ex) {
			System.err.println("Error opening file: " + nmapFile);
			ex.printStackTrace();
		}
		// System.out.println(nnmap);
		System.out.println("Finished reading in the neural net map");
	}

	/*
	 * loadMap: loads vectors from mapFile into the HashMap of vectors.
	 * The HashMap is filled with a key/value pair of the entity and a Counter
	 * that stores the vector/value pair of each vector.
	 */
	public void loadMap(String mapFile) {
		System.out.println("Reading in the vectors map from " + mapFile + "...");

		try {
                  //			@SuppressWarnings("resource")
                          //			BufferedReader file = new BufferedReader(new InputStreamReader(
                          //					new FileInputStream(new File(mapFile))));
                  BufferedReader reader;

                        // Zipped
                        if( mapFile.endsWith(".gz") ) {
                          InputStream in = new GZIPInputStream(new FileInputStream(new File(mapFile)));
                          reader = new BufferedReader(new InputStreamReader(in));
                        }
                        // Non-zipped
                        else
                          reader = new BufferedReader(new FileReader(mapFile));

			String line; // used to store the next line from mapFile
			int count = 0; // initialize counter

			while ((line = reader.readLine()) != null) {
				count++;

				if (count % 5000 == 0) { System.out.println("Total: " + count); }

				Counter<String> nw = new ClassicCounter<String>();
				String[] splLine = line.split("\t"); // split on tabs
				String ent = splLine[0].trim(); // grab entity from split line

				// Puts each vector/value pair of an entity into a counter
				if (splLine.length > 1) {
					if (splLine.length % 2 == 1) { // if splLine is odd ** SANITY CHECK
						for (int i = 1, n = splLine.length; i < n; i += 2) {
							Integer in = Integer.parseInt(splLine[i + 1]);
							nw.setCount(splLine[i], in);
						}
					}
				}

				nameVectors.put(ent, nw);

				//System.out.println(map.get(ent));
			}
		} catch (IOException ex) {
			System.err.println("Error opening file: " + mapFile);
			ex.printStackTrace();
		}

		System.out.println("Finished reading in the vector map\n");
	}

	/*
	 * loadCounts: loads counts from countFile into a HashMap containing
	 * the entity/count pairs for all entities in the file.
	 */
	public void loadCounts(String path) {
		System.out.println("Reading in the counts from " + path + "...");

		try {
                  @SuppressWarnings("resource")
                    BufferedReader reader;// = new BufferedReader(new InputStreamReader(
                        //					new FileInputStream(new File(countFile))));

                  // Zipped
                  if( path.endsWith(".gz") ) {
                    InputStream in = new GZIPInputStream(new FileInputStream(new File(path)));
                    reader = new BufferedReader(new InputStreamReader(in));
                  }
                  // Non-zipped
                  else
                    reader = new BufferedReader(new FileReader(path));
			
			String line;
			double num;

			while ((line = reader.readLine()) != null) {
				String[] splLine = line.split("\t");
				String ent = splLine[0].trim();
				if (splLine[1].equals("dummyWord"))
					num = Double.parseDouble(splLine[2].trim());
				else
					num = Double.parseDouble(splLine[1].trim());
				counts.put(ent, num);
			}
		} catch (IOException ex) {
			System.err.println("Error opening file: " + path);
			ex.printStackTrace();
		}

		System.out.println("Finished reading in the counts...\n");
	}

	/*
	 * *************************************
	 * END LOAD METHODS()
	 * *************************************
	 */

	/*
	 * assignByDesign: was used to generate the username file. No longer needed
	// makes testfile. then trains and tests off that test file
	public Map<String, Counter<String>> assignByDesign()
			throws FileNotFoundException, UnsupportedEncodingException {

		Map<String, Counter<String>> finalMap = new HashMap<String, Counter<String>>();
		String filename = "assignedByDesignTest.txt";
		PrintWriter writer = new PrintWriter(filename, "UTF-8");

		// make a testing file
		for (String entity : twitter_handles.keySet()) {
			for (String handle : twitter_handles.get(entity).keySet()) {
				// determine using weights
				if (twitter_handles.get(entity).getCount(handle) > 1.0) {
					if (!entity.contains("@") && entity.length() > 1) {
						writer.write(entity + ";" + handle + "\n");
					}
				}
			}
		}

		writer.close();
		System.exit(1);
		// trainAndTest("input.txt", filename);
		// trainAndTest("goldEntHandle", filename);

		/*
	 * double match = determination(entity, handle); if (match > .9) {
	 * System.err.println("handle: " + handle + "\tentity: " + entity); }


		return null;

		/*
	 * Map<String, Counter<String>> finalMap = new HashMap<String,
	 * Counter<String>>(); for (String handle : twitter_handles.keySet()) {
	 * int maxFreq=0; String maxStr = ""; for (String counterKey :
	 * twitter_handles.get(handle).keySet()) { int tmpCount =
	 * (int)twitter_handles.get(handle).getCount(counterKey); if (tmpCount >
	 * maxFreq) { maxFreq=tmpCount; maxStr=counterKey; } } Counter<String>
	 * newCounter = new ClassicCounter<String>();
	 * newCounter.incrementCount(maxStr, maxFreq); finalMap.put(handle,
	 * newCounter); } return finalMap;
	}
	 */

	/* assignByFrequency: was used to relate handles/entities based on frequency
	 * No longer needed
	// assigned a twitter handle to an entity based on frequency
	public Map<String, Counter<String>> assignByFrequency() {
		Map<String, Counter<String>> finalMap = new HashMap<String, Counter<String>>();
		for (String handle : twitter_handles.keySet()) {
			int maxFreq = 0;
			String maxStr = "";
			for (String counterKey : twitter_handles.get(handle).keySet()) {
				int tmpCount = (int) twitter_handles.get(handle).getCount(
						counterKey);
				if (tmpCount > maxFreq) {
					maxFreq = tmpCount;
					maxStr = counterKey;
				}
			}
			Counter<String> newCounter = new ClassicCounter<String>();
			newCounter.incrementCount(maxStr, maxFreq);
			finalMap.put(handle, newCounter);
		}
		return finalMap;
	}
	 */

	/* assignRandomly: randomly assign handles to entities. No longer needed.
	// assigns twitter handles to entities randomly
	public Map<String, Counter<String>> assignRandomly() {
		Map<String, Counter<String>> finalMap = new HashMap<String, Counter<String>>();
		Random rand = new Random();

		for (String handle : twitter_handles.keySet()) {
			int max = twitter_handles.get(handle).size();
			int min = 0;
			int randomNum = rand.nextInt(max - min);
			int ii = 0;
			for (String counterKey : twitter_handles.get(handle).keySet()) {
				if (ii == randomNum) {
					Counter<String> newCounter = new ClassicCounter<String>();
					newCounter.incrementCount(counterKey,
							twitter_handles.get(handle).getCount(counterKey));
					finalMap.put(handle, newCounter);
					break;
				}
				ii++;
			}
		}

		return finalMap;
	}
	 */

	/* check_handles: used for debugging purposes
	 * public void check_handles(String option) throws FileNotFoundException,
	 * UnsupportedEncodingException { List<String> allDays = new
	 * ArrayList<String>(); //limit the files for(String name :
	 * Directory.getFilesSorted(et.entityStats)){ allDays.add(name); }
	 * 
	 * //combine all the maps into twitter_handles... this may cause memory
	 * trouble with more maps int count=0; for (String day : allDays) { if
	 * (count > numFiles) { break; } Map<String, Counter<String>> tmp=null; if
	 * (day.contains("entity_to_handle")) { tmp =
	 * et.deserialize(twitter_handles, et.entityStats+day); count++; }
	 * 
	 * if (tmp != null) { for (String entity : tmp.keySet()) { if
	 * (tmp.get(entity).size() != 0 && twitter_handles.containsKey(entity)) {
	 * twitter_handles
	 * .get(entity).incrementCount(get_counter_string(tmp.get(entity)),
	 * (get_counter_count(tmp.get(entity)))); } else if (tmp.get(entity).size()
	 * != 0) { twitter_handles.put(entity, tmp.get(entity)); } } } }
	 * 
	 * //make new map based on option passed in as parameter Map<String,
	 * Counter<String>> tmpFinalMap = null; if (option.equals("design")) {
	 * tmpFinalMap = assignByDesign(); } else if (option.equals("rand")) {
	 * tmpFinalMap = assignRandomly(); } else {
	 * System.err.println("assigning by frequency"); tmpFinalMap =
	 * assignByFrequency(); }
	 * 
	 * //print entity to handle relationship for (String entity :
	 * tmpFinalMap.keySet()) { for (String handle :
	 * tmpFinalMap.get(entity).keySet()) { System.err.println("key: " + entity +
	 * "  ====>  " + handle + "  " + tmpFinalMap.get(entity).getCount(handle));
	 * } }
	 * 
	 * }
	 */

	/* makeTrainingOrTestingFile: returns filename of the training file
	 * // the training file should be taken in by GetWeights
	 * private String makeTrainingOrTestingFile(String inputFile, boolean
	 * testing) { String finalFile; if (testing) { finalFile =
	 * "convertedTest.txt"; } else { finalFile = "featureVectors"; }
	 * BufferedReader br = null; int featuresSize = 7; //should make this
	 * automatic System.err.println("working dir: " +
	 * System.getProperty("user.dir"));
	 * 
	 * try { String curLine; br = new BufferedReader(new FileReader(inputFile));
	 * boolean[] boolArray = new boolean[featuresSize]; PrintWriter writer = new
	 * PrintWriter(finalFile, "UTF-8"); boolean trueExamples=true;
	 * 
	 * writer.write("-1," + featuresSize + "\n"); //the first line rule should
	 * probably be changed... applies to train and test while ((curLine =
	 * br.readLine()) != null) { //continue reading until end of file
	 * //System.err.println("curLine: " + curLine); if
	 * (curLine.trim().equals("!@#BREAK$%^")) {trueExamples=false;continue;}
	 * //now we only get false datapoints
	 * 
	 * //get the values of each feature String handle = curLine.split(";")[1];
	 * //System.err.println("curLine: " + curLine); String ent =
	 * curLine.split(";")[0];
	 * 
	 * if (testing) {entityHandlePairs.add(ent + " : " + handle);} //sketchy
	 * boolArray[0] = hasSemiAcronym(ent, handle); boolArray[1] =
	 * is_acronym(ent, handle); boolArray[2] = first_name_substring(ent,
	 * handle); boolArray[3] = last_name_substring(ent, handle); //boolArray[4]
	 * = firstThreeLetterMatch(ent, handle); boolArray[5] = capitalization(ent,
	 * handle); //boolArray[6] = percentSubstring(ent, handle);
	 * 
	 * 
	 * for (int ii=0; ii<boolArray.length; ii++) { boolean b = boolArray[ii]; if
	 * (b) { writer.write("1"); } else { writer.write("0"); } if (ii !=
	 * boolArray.length-1) { writer.write(","); } } if (!testing) { if
	 * (trueExamples) {writer.write(",1\n");} // this may need to change!!! *I'm
	 * talking about manual inputs* else {writer.write(",0\n");} // this may
	 * need to change!!! *I'm talking about manual inputs* } else {
	 * writer.write("\n"); } } writer.close(); } catch (IOException e) {
	 * e.printStackTrace(); System.exit(1); //does this happen??? I think sooo }
	 * 
	 * return finalFile; }
	 */

	/**
	 * Given an entity's name and its handle, build features based on character
	 * overlap. Comment-out features to turn them off. If wordVector() or neuralNetVector()
	 * methods are off, turn off their respective load() methods. If wordVectorAverage() is being
	 * used, uncomment the averageVector() calls in train() and test(). 
	 * 
	 * @param entity
	 *            Full name of the entity.
	 * @param handle
	 *            Shortened nickname of the entity.
	 * @return A Counter of all the features and their feature values.
	 * @throws IOException
	 */
	public Counter<String> featsIntoCounter(String entity, String handle)
			throws IOException {
	    Counter<String> c = new ClassicCounter<String>();

	    /*
	    // On dev set, including invertEdit shows no change in accuracy.
	    c.incrementCount("editDistance", computeEditDistance(entity, handle)); // increases
	    //c.incrementCount("invertEditDistance", computeInverseEditDistance(entity, handle));
    c.incrementCount("lastNameSubstring", last_name_substring(entity, handle)); // increases
    c.incrementCount("firstNameSubstring", first_name_substring(entity, handle)); // increases
    c.incrementCount("percentSubstring", percentSubstring(entity, handle)); // increases
    c.incrementCount("firstNLetterMatch", firstNLetterMatch(entity, handle)); // increases
    c.incrementCount("lastNLetterMatch", lastNLetterMatch(entity, handle)); // increases
    c.incrementCount("hasSemiAcr", hasSemiAcronym(entity, handle)); // increases
    c.incrementCount("isAcr", is_acronym(entity, handle)); // increases
    c.incrementCount("capitalAcronym", capitalAcronym(entity, handle)); // increases
    c.incrementCount("containsWholeEntity", containsWholeEntity(entity, handle)); // increases
    c.incrementCount("capitalSubstring", capitalSubstring(entity, handle)); // increases
    c.incrementCount("capitalization", capitalization(entity, handle)); // increases
    c.incrementCount("exactMatch", exactMatch(entity,handle));  // increases
    c.incrementCount("reverseSubstring", reverseSubstring(entity, handle)); // slight increase

    //c.incrementCount("oneWordSubstring", oneWordSubstring(entity,handle)); // decreases accuracy
    */
    if( usingVecs ) {
	//c.incrementCount("neuralNetVector", dl4j(entity,handle)); // testing
	c.incrementCount("wordVectorBinary", wordVectorBinary(entity, handle)); // Least effective wordVector
	c.incrementCount("wordVectorCosine", wordVectorCosine(entity, handle)); // Most effective wordVector
	c.incrementCount("wordVectorCosineAverage", wordVectorCosineAverage(entity, handle)); // Meh.
    }
    
    return c;

		/*
		 * 
		 * wordVectorCosine() seems to provide the best results. Although the use
		 * of all three wordVector features typically gives the best results on high
		 * countThreshold tests.
		 * 
		 * wordVectorCosineAverage() is the only feature that uses averageVector(), turn
		 * it off if you are not using it by commenting out the call in train() and test().
		 * 
		 * If you are not using certain features make sure you turn off load
		 * loadCounts, loadMap, or loadDL4J. loadMap and averaging in particular will make
		 * this program this take for ever.
		 * 
		 */
	}

	/**
	 * Reads all entity/handle/label datums from a file. File format:
	 * entity;handle;label
	 * 
	 * @param filename "usernames-training.txt"
	 *            Path to the file.
	 * @return A list of entity/handle pairs.
	 */
	private List<EntityHandle> getEntityData(String filename) {
		BufferedReader br = null;
//		System.err.println("working dir: " + System.getProperty("user.dir"));
		List<EntityHandle> handles = new LinkedList<EntityHandle>();

		try {

			String curLine;
			br = new BufferedReader(new FileReader(filename));

			while ((curLine = br.readLine()) != null) {
				String entity = curLine.split(";")[0];
				String handle = curLine.split(";")[1];
				String label = curLine.split(";")[2];
				handles.add(new EntityHandle(entity,handle,label.equals("correct")));
			}

			br.close();

		} catch (Exception e) {
			e.printStackTrace();
			System.err.println("WE HAVE AN ERROR IN GETDATA(FILENAME)");
			System.exit(1);
		}

		return handles;
	}


	/**
	 * Expects a file with one entity/username/label per line: e.g., Manny
	 * Ortiz;@manuelooo_26;correct Trains a classifier based on the file's
	 * pairs. Utilizes featsIntoCounter.
	 * 
	 * @param filename
	 *            Path to the training file.
	 * @throws IOException
	 */
	public void train(String filename) throws IOException {
		System.out.println("Started train() function...\n");

		//averageVector(filename); // load avgMap with training set cosine averages

		// Get the data from usernames-training.txt
		List<EntityHandle> entities = getEntityData(filename);
		RVFDataset<String, String> data = new RVFDataset<String, String>();

		// Quantify features based off of the dataset
		for (EntityHandle eh : entities) {
			Counter<String> tmpCount = featsIntoCounter(eh.entity, eh.handle);
			data.add(new RVFDatum<String, String> (
					tmpCount, eh.correct ? "match" : "!match"));
		}

		System.err.println("Finished making dataset.\n");

		// Train the linear classifier
		LinearClassifierFactory<String, String> linearFactory = new LinearClassifierFactory<String, String>();
		myClassifier = linearFactory.trainClassifier(data);
		System.err.println("weight: " + myClassifier.toAllWeightsString());
		System.err.println("Finished making training.\n");
	}

	/**	
	 * test(): This method iterates over all entity/handle pairs from the test set and 
	 * predicts a match using isMatch(). Results are stored in the fields instantiated
	 * at the top of the method.
	 * 
	 * @param filename
	 * 		path to usernames-test.txt file
	 * 
	 * @throws IOException
	 * 
	 */
	public void test(String filename) throws IOException {

		//averageVector(filename); // load avgMap with average cosine values from test set

		// Output and result variables. One for each iteration of countThreshold
		int correct1 = 0, correct2 = 0, correct3 = 0;
		int incorrect1 = 0, incorrect2 = 0, incorrect3 = 0;
		double acc1 = 0, acc2 = 0, acc3 = 0;
		double matchp1 = 0, matchp2 = 0, matchp3 = 0;
		double matchr1 = 0, matchr2 = 0, matchr3 = 0;
		double nmatch1 = 0, nmatch2 = 0, nmatch3 = 0;
		double nmatchr1 = 0, nmatchr2 = 0, nmatchr3 = 0;
		int vecCount1 = 0, vecCount2 = 0, vecCount3 = 0;
		int totVec1 = 0, totVec2 = 0, totVec3 = 0;
    double sumavg1 = 0.0, sumavg3 = 0.0;

		// Make three iterations using three different countThresholds, change as desired
		for (int i = 0; i < 3; i++) {
			int countThreshold = 0;
			vecCount = 0; totVecCount = 0; // reset vecCounts each iteration
			if (i == 0) { countThreshold = 0; }
			if (i == 1) { countThreshold = 50; }
			if (i == 2) { countThreshold = 100; }

			// Initialize counters for testing
			int truePos = 0, falsePos = 0, trueNeg = 0, falseNeg = 0, total = 0;

			// Load usernames-testing.txt into entities
			List<EntityHandle> entities = getEntityData(filename);
			System.err.println("Finished getting testing data for count threshold: " + countThreshold + "!\n");

			// Debugging: finds the total number of elements in count with a value above
			// the count threshold
			//				int counter = 0;
			//				for( String key : counts.keySet() )
			//					if (entry.getValue() >= countThreshold) { counter++; }
			//				
			//				System.out.println("Count Threshold: " + countThreshold);
			//				System.out.println("Total number of counts: " + counter);
			//				counter = 0;
			//				int threshCounter = 0;
			//				
			//				System.out.println("Number of entities to test: " + entities.size());


			for (EntityHandle eh : entities) {

				// Debugging: compares elements in "counts" with elements in "entities"
				//					for( String key : counts.keySet() ) {
				//						if (key.equals(eh.entity)) { 
				//							if (counts.get(key) >= countThreshold) { threshCounter++; }
				//							counter++;
				//						}
				//					}

				// Skip entity if seen infrequently.
				if (i > 0 && (!counts.containsKey(eh.entity) || counts.get(eh.entity) <= countThreshold))
					continue;

				boolean match = isMatch(eh.entity, eh.handle);

				// DEBUG
				double edit = computeEditDistance(eh.entity, eh.handle);
		    double avg = edit / (eh.entity.length() > 0.0 ? eh.entity.length() : 1.0);
		    if( countThreshold == 0 ) sumavg1 += avg;
		    if( countThreshold == 100 ) sumavg3 += avg;
		    
				//System.out.println("TEST: " + eh.entity + "\t" + eh.handle + "\t" + match + "\t" + eh.correct);
				if (match && eh.correct) 
				  truePos++;
				else if (match && !eh.correct) {
					//System.out.println("FalsePos:\t" + eh.entity + "\t" + eh.handle + "\t" + eh.correct);
					falsePos++;
				}
				else if (!match && !eh.correct) 
				  trueNeg++;
				else if (!match && eh.correct) {
				  //System.out.println("FalseNeg:\t" + eh.entity + "\t" + eh.handle + "\t" + eh.correct);
					falseNeg++;
				}
				total++;
			}

			// Debugging
			//System.out.println("Entity-Counter Matches: " + counter);
			//System.out.println("Entity-Counter > Threshold: " + threshCounter + "\n");

			// Fills fields with match data
			if (i == 0) {
				correct1 = truePos + trueNeg;
				incorrect1 = falsePos + falseNeg;
				acc1 = (truePos + trueNeg) / (double)total;
				matchp1 = (truePos / (double)(truePos + falsePos));
				matchr1 = (truePos / (double)(truePos + falseNeg));
				nmatch1 = (trueNeg / (double)(trueNeg + falseNeg));
				nmatchr1= (trueNeg / (double)(trueNeg + falsePos));
				vecCount1 = vecCount; totVec1 = totVecCount;
				sumavg1 /= (double)total;
			} else if (i == 1) {
				correct2 = truePos + trueNeg;
				incorrect2 = falsePos + falseNeg;
				acc2 = (truePos + trueNeg) / (double)total;
				matchp2 = (truePos / (double)(truePos + falsePos));
				matchr2 = (truePos / (double)(truePos + falseNeg));
				nmatch2 = (trueNeg / (double)(trueNeg + falseNeg));
				nmatchr2= (trueNeg / (double)(trueNeg + falsePos));
				vecCount2 = vecCount; totVec2 = totVecCount;
			} else if (i == 2) {
				correct3 = truePos + trueNeg;
				incorrect3 = falsePos + falseNeg;
				acc3 = (truePos + trueNeg) / (double)total;
				matchp3 = (truePos / (double)(truePos + falsePos));
				matchr3 = (truePos / (double)(truePos + falseNeg));
				nmatch3 = (trueNeg / (double)(trueNeg + falseNeg));
				nmatchr3= (trueNeg / (double)(trueNeg + falsePos));
				vecCount3 = vecCount; totVec3 = totVecCount;
				sumavg3 /= (double)total;
			}
		}

		
		// Prints the output/analysis of the program
		System.out.println("CountThreshold:\t\t0\t\t50\t\t100");
		System.out.println("Correct:\t" + "\t" + correct1 + "\t\t" + correct2 + "\t\t" + correct3);
		System.out.println("Incorrect:\t" + "\t" + incorrect1 + "\t\t" + incorrect2 + "\t\t" + incorrect3);
		System.out.printf("Accuracy:\t\t%.5f\t\t%.5f\t\t%.5f\n", acc1, acc2, acc3);
		System.out.printf("Match Precision:\t%.5f\t\t%.5f\t\t%.5f\n", matchp1, matchp2, matchp3);
		System.out.printf("Match Recall:\t\t%.5f\t\t%.5f\t\t%.5f\n", matchr1, matchr2, matchr3);
		System.out.printf("NonMatch Precision:\t%.5f\t\t%.5f\t\t%.5f\n", nmatch1, nmatch2, nmatch3);
		System.out.printf("NonMatch Recall:\t%.5f\t\t%.5f\t\t%.5f\n", nmatchr1, nmatchr2, nmatchr3);
		System.out.println("Vector Count:\t" + "\t" + vecCount1 + " / " + totVec1 + "\t\t" + 
				vecCount2 + " / " + totVec2 + "\t\t" + vecCount3 + " / " + totVec3 + "\n");
		System.out.printf("Average edit distance:\t%.3f\t---\t%.3f\n", sumavg1, sumavg3);
	}

	/**
	 * Uses the classifier to determine if the given entity and handle are a
	 * match.
	 * 
	 * @throws IOException
	 */
	public boolean isMatch(String entity, String handle) throws IOException {
		Counter<String> tmpCount = featsIntoCounter(entity, handle);
		@SuppressWarnings({ "deprecation", "rawtypes", "unchecked" })
		Counter<String> scores = myClassifier.scoresOf(new RVFDatum(tmpCount,"match"));
		Counters.logNormalizeInPlace(scores);

		for (String label : scores.keySet())
			scores.setCount(label, Math.exp(scores.getCount(label)));

		String label = Counters.argmax(scores);

		if (label.equals("match")) { return true; }

		return false;
	}

	/**
	 * Compares entity/handle and returns a positive match only if the two are
	 * identical. Used to get a baseline on accuracy before the addition of features.
	 */
	public void exactMatchTest(String filename, int type) throws IOException {

		int correct1 = 0, correct2 = 0, correct3 = 0;
		int incorrect1 = 0, incorrect2 = 0, incorrect3 = 0;
		double acc1 = 0, acc2 = 0, acc3 = 0;
		double matchp1 = 0, matchp2 = 0, matchp3 = 0;
		double matchr1 = 0, matchr2 = 0, matchr3 = 0;
    double nmatch1 = 0, nmatch2 = 0, nmatch3 = 0;
    double nmatchr1 = 0, nmatchr2 = 0, nmatchr3 = 0;

		for (int i = 0; i < 3; i++) {
			int countThreshold = 0;
			if (i == 0) { countThreshold = 0; }
			if (i == 1) { countThreshold = 50; }
			if (i == 2) { countThreshold = 100; }

			// Initialize counters for testing
			int truePos = 0, falsePos = 0, trueNeg = 0, falseNeg = 0, total = 0;

			// Load usernames-testing.txt into entities
			List<EntityHandle> entities = getEntityData(filename);
//			System.err.println("Finished getting testing data for count: " + countThreshold + "!\n");

			for (EntityHandle eh : entities) {

				// Skip entity if seen infrequently.
				if (i > 0 && (!counts.containsKey(eh.entity) || counts.get(eh.entity) <= countThreshold))
					continue;

				int match = 1;
				if( type == 1 )
				  match = exactMatchAlphaNumeric(eh.entity, eh.handle);
				else if( type == 2 )
				  match = exactMatch(eh.entity, eh.handle);
				else if( type == 3 )
				  match = exactMatchStartString(eh.entity, eh.handle, 5);

				//System.out.println("TEST: " + eh.entity + "\t" + eh.handle + "\t" + match + "\t" + eh.correct);
				if (match == 1 && eh.correct) { truePos++;
//				System.out.println(eh.entity + "\t" + eh.handle + "\t" + match + "\t" + eh.correct);
				}
				else if (match == 1 && !eh.correct) {
					//System.out.println("FalsePos:\t" + eh.entity + "\t" + eh.handle + "\t" + eh.correct);
					falsePos++;
				}
				else if (match == 0 && !eh.correct) { trueNeg++; }
				else if (match == 0 && eh.correct) {
					//System.out.println("FalseNeg:\t" + eh.entity + "\t" + eh.handle + "\t" + eh.correct);
					falseNeg++;
				}
				total++;
			}

			// Debugging
			//System.out.println("Entity-Counter Matches: " + counter);
			//System.out.println("Entity-Counter > Threshold: " + threshCounter + "\n");

		   // Fills fields with match data
      if (i == 0) {
        correct1 = truePos + trueNeg;
        incorrect1 = falsePos + falseNeg;
        acc1 = (truePos + trueNeg) / (double)total;
        matchp1 = (truePos / (double)(truePos + falsePos));
        matchr1 = (truePos / (double)(truePos + falseNeg));
        nmatch1 = (trueNeg / (double)(trueNeg + falseNeg));
        nmatchr1= (trueNeg / (double)(trueNeg + falsePos));
      } else if (i == 1) {
        correct2 = truePos + trueNeg;
        incorrect2 = falsePos + falseNeg;
        acc2 = (truePos + trueNeg) / (double)total;
        matchp2 = (truePos / (double)(truePos + falsePos));
        matchr2 = (truePos / (double)(truePos + falseNeg));
        nmatch2 = (trueNeg / (double)(trueNeg + falseNeg));
        nmatchr2= (trueNeg / (double)(trueNeg + falsePos));
      } else if (i == 2) {
        correct3 = truePos + trueNeg;
        incorrect3 = falsePos + falseNeg;
        acc3 = (truePos + trueNeg) / (double)total;
        matchp3 = (truePos / (double)(truePos + falsePos));
        matchr3 = (truePos / (double)(truePos + falseNeg));
        nmatch3 = (trueNeg / (double)(trueNeg + falseNeg));
        nmatchr3= (trueNeg / (double)(trueNeg + falsePos));
      }
		}

		// Prints the output/analysis of the program
		System.out.print("BASELINE EXACT MATCH TEST");
    if( type == 2 ) System.out.println("(lowercased no-white)");
		if( type == 1 ) System.out.println("(stripped alpha only)");
		if( type == 3 ) System.out.println("(stripped alpha only, first 5 chars)");
//		System.out.println("CountThreshold:\t\t0\t\t\t50\t\t\t100");
//		System.out.println("Correct:\t" + "\t" + correct1 + "\t\t\t" + correct2 + "\t\t\t" + correct3);
//		System.out.println("Incorrect:\t" + "\t" + incorrect1 + "\t\t\t" + incorrect2 + "\t\t\t" + incorrect3);
//		System.out.println("Accuracy:\t" + "\t" + acc1 + "\t" + acc2 + "\t" + acc3);
//		System.out.println("MatchPrecision:\t" + "\t" + matchp1 + "\t" + matchp2 + "\t" + matchp3);
//		System.out.println("NonMatchPrecision:" + "\t" + nmatch1 + "\t" + nmatch2 + "\t" + nmatch3);
		
    System.out.println("CountThreshold:\t\t0\t\t50\t\t100");
    System.out.println("Correct:\t" + "\t" + correct1 + "\t\t" + correct2 + "\t\t" + correct3);
    System.out.println("Incorrect:\t" + "\t" + incorrect1 + "\t\t" + incorrect2 + "\t\t" + incorrect3);
    System.out.printf("Accuracy:\t\t%.5f\t\t%.5f\t\t%.5f\n", acc1, acc2, acc3);
    System.out.printf("Match Precision:\t%.5f\t\t%.5f\t\t%.5f\n", matchp1, matchp2, matchp3);
    System.out.printf("Match Recall:\t\t%.5f\t\t%.5f\t\t%.5f\n", matchr1, matchr2, matchr3);
    System.out.printf("NonMatch Precision:\t%.5f\t\t%.5f\t\t%.5f\n", nmatch1, nmatch2, nmatch3);
    System.out.printf("NonMatch Recall:\t%.5f\t\t%.5f\t\t%.5f\n", nmatchr1, nmatchr2, nmatchr3);
	}

	/*
	 * Dumb baseline classifier... match if any of the words in the entity are
	 * in the handle (all lowercase)

	private void dumbTest(String filename) {
		List<EntityHandle> entities = getEntityData(filename);

		for (EntityHandle eh : entities) {
			String[] entityTokens = eh.entity.split("\\s+");
			boolean match = false;
			for (String entityToken : entityTokens) {
				if (eh.handle.toLowerCase().contains(entityToken.toLowerCase())) {
					match = true;
					break;
				}
			}
			if (match)
				System.err.println(eh.entity + ";" + eh.handle + ";match");
			else
				System.err.println(eh.entity + ";" + eh.handle + ";!match");
		}
	}

	private void testFeatures(String entity, String handle) {
		String e = entity;
		String h = handle;
		System.err.println("hasSemiAcr:  " + hasSemiAcronym(e, h));
		System.err.println("isAcr:  " + is_acronym(e, h));
		System.err.println("firstNameSubstring:  " + first_name_substring(e, h));
		System.err.println("lastNLetterMatch:  " + firstNLetterMatch(e, h));
		System.err.println("lastNLetterMatch:  " + lastNLetterMatch(e, h));
		System.err.println("capitalization:  " + capitalization(e, h));
		System.err.println("percentSubstring:  " + percentSubstring(e, h));
		System.err.println("editDistance:  " + computeEditDistance(e, h));
		System.err.println("lastNameSubstring:  " + last_name_substring(e, h));
		System.err.println("containsWholeEntity:  " + containsWholeEntity(e, h));
	}
	 */

	public static void main(String[] args) throws IOException {

		if(args.length != 5) {
			System.out.println("USAGE: ./TwitterHandles <trainingFile> <testingFile> <vectorFile> <countFile> <matrixFile>");
		} else {
			// Train and Test.
			TwitterHandles handler = new TwitterHandles();
			handler.load(args[2], args[3], args[4]); // loads the 3 vector files into maps w/ ent,count pairs
			handler.train(args[0]);
			handler.test(args[1]);

			// Baseline tests.
      handler.exactMatchTest(args[1], 2);
      handler.exactMatchTest(args[1], 1);
      handler.exactMatchTest(args[1], 3);
		}

		// t.dumbTest("testing2");
		// t.testFeatures("the Future 2", "@thebanning");
		// public LinearClassifier<String, String> myClassifier;
		// String testingFile = "test.txt";
		// t.trainAndTest(trainingFile, testingFile);*/
	}
}
